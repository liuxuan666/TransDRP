{
 "unlabeled": {"batch_size": 128, 
     "lr": 0.0001, 
     "pretrain_num_epochs": 300,
     "train_num_epochs": 300
     }, 
 "labeled": {"classifier_hidden_dims": [64, 32, 16], 
     "batch_size": 64, 
     "lr": 0.0001, 
     "train_num_epochs": 600,
     "uda_num_epochs": 300
     }, 
 "encoder_hidden_dims": [512, 256, 64],
 "decoder_hidden_dims": [128, 256, 512],
 "latent_dim": 64, 
 "drop": 0.2
}